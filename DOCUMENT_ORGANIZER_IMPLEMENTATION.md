# Guía de Implementación: Sistema de Organización de Documentos

## Resumen Ejecutivo

Sistema de organización en 2 fases para preorganizar documentos antes de la extracción guiada:
- **Fase A (Barata)**: Clasificación heurística con fallback a LLM económico, renombrado y staging
- **Fase B (Guiada)**: Extracción de campos clave usando el sistema existente, consolidación y renombrado final

## Arquitectura General

```
data/
├── uploads/
│   ├── renombre_de_documentos/      # Staging temporal
│   │   └── <YYYYMMDD-HHMMSS>/      # Carpeta timestamped
│   │       ├── mapping.json         # Mapeo de archivos
│   │       └── archivos renombrados # <idx>__<tipo>__<ruta>__<nombre>.<ext>
│   └── <ASEGURADO + SINIESTRO>/     # Carpeta final tras Fase B
```

## Componentes Existentes Reutilizables

### 1. Detección y Clasificación (ai_field_extractor.py)
- `_detect_document_type()`: Detección heurística por keywords (línea 334)
- `_determine_route()`: Determina si usar OCR o Direct AI (línea 491)
- Ya tiene indicadores básicos para: póliza, factura, denuncia, peritaje, carta_porte

### 2. OCR y Cache (ocr_cache.py, azure_ocr.py)
- `OCRCacheManager`: Gestión de cache con hash SHA256
- `AzureOCRProcessor`: Procesamiento OCR con Azure
- Cache persiste resultados en DB SQLite

### 3. Validación y Normalización (validators.py)
- `FieldValidator`: Valida formatos (fechas, montos, regex)
- `DataValidator`: Normaliza y limpia datos
- Reglas en `FIELD_VALIDATION_RULES` de settings.py

### 4. Configuración (settings.py)
- `DOCUMENT_FIELD_MAPPING`: Mapeo tipo → campos permitidos
- `FIELD_SYNONYMS`: Sinónimos para búsqueda
- `FIELD_VALIDATION_RULES`: Reglas de validación

### 5. Extracción Guiada (extraction_prompts.py)
- `build_guided_extraction_prompt()`: Prompts optimizados por tipo
- Sistema ya implementado y funcionando

## Fase A: Clasificación y Staging Barato

### A.1 Flujo Principal

```python
async def organize_documents_phase_a(
    input_folder: Path,
    staging_base: Path = Path("data/uploads/renombre_de_documentos"),
    use_llm_fallback: bool = True
) -> tuple[Path, Dict[str, Any]]:
    """
    Fase A: Clasificación barata y organización en staging
    
    Returns:
        - staging_folder: Ruta de la carpeta staging creada
        - mapping: Diccionario con el mapeo de archivos
    """
    # 1. Crear carpeta staging con timestamp
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    staging_folder = staging_base / timestamp
    staging_folder.mkdir(parents=True, exist_ok=True)
    
    # 2. Descubrir archivos soportados
    supported_extensions = {'.pdf', '.jpg', '.jpeg', '.png', '.docx', '.xlsx', '.csv'}
    files = [f for f in input_folder.iterdir() 
             if f.suffix.lower() in supported_extensions]
    
    # 3. Clasificar y renombrar
    mapping = {"files": [], "metadata": {"timestamp": timestamp}}
    classifier = DocumentClassifier()
    
    for idx, file_path in enumerate(sorted(files), 1):
        # Determinar ruta (OCR vs Direct AI)
        route = _determine_route_for_file(file_path)
        
        # Obtener texto para clasificación
        sample_text = await _get_sample_text(file_path, route)
        
        # Clasificar documento
        doc_type, confidence, reasons = await classifier.classify(
            sample_text, 
            file_path.name,
            use_llm_fallback=use_llm_fallback
        )
        
        # Generar nombre nuevo
        route_label = "OCR" if route == "ocr_text" else "VIS"
        new_name = f"{idx:03d}__{doc_type}__{route_label}__{file_path.stem[:50]}{file_path.suffix}"
        new_path = staging_folder / new_name
        
        # Copiar archivo
        shutil.copy2(file_path, new_path)
        
        # Registrar en mapping
        mapping["files"].append({
            "original": str(file_path),
            "staged": str(new_path),
            "document_type": doc_type,
            "route": route,
            "confidence": confidence,
            "reasons": reasons,
            "index": idx
        })
    
    # 4. Guardar mapping.json
    mapping_file = staging_folder / "mapping.json"
    with open(mapping_file, "w", encoding="utf-8") as f:
        json.dump(mapping, f, indent=2, ensure_ascii=False)
    
    return staging_folder, mapping
```

### A.2 Clasificador de Documentos

```python
class DocumentClassifier:
    """Clasificador híbrido: heurística primero, LLM si necesario"""
    
    def __init__(self):
        self.config = ExtractionConfig()
        self.type_indicators = {
            "poliza_de_la_aseguradora": [
                "póliza", "vigencia", "cobertura", "prima", "asegurado",
                "condiciones generales", "suma asegurada"
            ],
            "informe_preliminar_del_ajustador": [
                "ajustador", "informe preliminar", "inspección", "evaluación",
                "siniestro", "recomendaciones", "conclusiones"
            ],
            "carta_de_reclamacion_formal_a_la_aseguradra": [
                "reclamación", "reclamo", "solicito", "requiero", 
                "monto reclamado", "indemnización"
            ],
            "carpeta_de_investigacion": [
                "investigación", "carpeta", "averiguación", "ministerio público"
            ],
            "denuncia": [
                "denuncia", "querella", "delito", "hechos delictivos"
            ],
            "factura": [
                "factura", "cfdi", "subtotal", "iva", "total", "rfc"
            ],
            "carta_porte": [
                "carta porte", "transportista", "remitente", "destinatario",
                "guía", "embarque"
            ],
            "peritaje": [
                "peritaje", "dictamen", "perito", "valuación", "daños"
            ]
        }
        
        # Guía compacta para LLM
        self.llm_guide = self._build_llm_guide()
    
    async def classify(
        self, 
        sample_text: str, 
        filename: str,
        use_llm_fallback: bool = True
    ) -> tuple[str, float, list]:
        """
        Clasifica documento
        Returns: (document_type, confidence, reasons)
        """
        # 1. Intentar clasificación heurística
        doc_type, confidence, reasons = self._heuristic_classify(
            sample_text, filename
        )
        
        # 2. Si confianza baja y LLM habilitado
        if confidence < 0.6 and use_llm_fallback and doc_type == "otro":
            doc_type, confidence, reasons = await self._llm_classify(
                sample_text[:1500],  # Limitar a 1500 chars
                filename
            )
        
        return doc_type, confidence, reasons
    
    def _heuristic_classify(
        self, text: str, filename: str
    ) -> tuple[str, float, list]:
        """Clasificación por keywords"""
        text_lower = text.lower()
        filename_lower = filename.lower()
        
        scores = {}
        matches = {}
        
        for doc_type, keywords in self.type_indicators.items():
            found_keywords = []
            score = 0
            
            for keyword in keywords:
                if keyword in text_lower:
                    score += 2  # Peso mayor para contenido
                    found_keywords.append(f"texto:'{keyword}'")
                elif keyword in filename_lower:
                    score += 1  # Peso menor para nombre
                    found_keywords.append(f"nombre:'{keyword}'")
            
            if score > 0:
                scores[doc_type] = score
                matches[doc_type] = found_keywords
        
        if not scores:
            return "otro", 0.0, ["No se encontraron indicadores"]
        
        # Mejor match
        best_type = max(scores, key=scores.get)
        best_score = scores[best_type]
        
        # Calcular confianza (normalizada)
        max_possible = len(self.type_indicators[best_type]) * 2
        confidence = min(best_score / max_possible, 1.0)
        
        return best_type, confidence, matches[best_type]
    
    async def _llm_classify(
        self, sample_text: str, filename: str
    ) -> tuple[str, float, list]:
        """Clasificación con LLM económico (gpt-4o-mini)"""
        
        prompt = f"""Clasifica este documento de siniestro de seguros.

TIPOS PERMITIDOS:
{self.llm_guide}

DOCUMENTO:
Archivo: {filename}
Contenido (muestra):
{sample_text}

Responde SOLO con JSON:
{{
  "document_type": "tipo_exacto_de_la_lista",
  "confidence": 0.0-1.0,
  "reasons": ["razón 1", "razón 2"]
}}"""

        try:
            client = AsyncOpenAI()
            response = await client.chat.completions.create(
                model="gpt-4o-mini",  # Modelo económico
                messages=[{"role": "user", "content": prompt}],
                temperature=0.1,
                max_tokens=200
            )
            
            result = json.loads(response.choices[0].message.content)
            
            # Validar que el tipo esté en los permitidos
            if result["document_type"] not in self.type_indicators:
                result["document_type"] = "otro"
                result["confidence"] *= 0.5
            
            return (
                result["document_type"],
                result["confidence"],
                result["reasons"]
            )
            
        except Exception as e:
            logger.warning(f"Error en clasificación LLM: {e}")
            return "otro", 0.0, [f"Error LLM: {str(e)}"]
    
    def _build_llm_guide(self) -> str:
        """Construye guía compacta para LLM"""
        guide_lines = []
        for doc_type, keywords in self.type_indicators.items():
            # Tomar solo los 3 keywords más representativos
            key_words = keywords[:3]
            guide_lines.append(
                f"- {doc_type}: contiene {', '.join(key_words)}"
            )
        guide_lines.append("- otro: documentos que no encajan en categorías anteriores")
        return "\n".join(guide_lines)
```

### A.3 Helpers para Fase A

```python
async def _get_sample_text(file_path: Path, route: str, max_chars: int = 2000) -> str:
    """
    Obtiene texto de muestra para clasificación
    Usa cache si existe, sino extrae rápido
    """
    # Intentar cache primero
    cache_manager = OCRCacheManager()
    doc_id, file_hash = ensure_document_registered("temp_classify", str(file_path))
    
    cached = try_get_cached_ocr(doc_id, file_hash)
    if cached:
        return cached.get("text", "")[:max_chars]
    
    # Extraer según tipo
    if file_path.suffix.lower() == '.pdf':
        # Para PDFs, intentar extracción de texto nativo primero
        try:
            import PyPDF2
            with open(file_path, 'rb') as f:
                reader = PyPDF2.PdfReader(f)
                text = ""
                for page in reader.pages[:2]:  # Solo primeras 2 páginas
                    text += page.extract_text()
                return text[:max_chars]
        except:
            pass
    
    # Si es imagen o PDF sin texto, usar OCR sample
    if route == "ocr_text":
        # OCR rápido de página 1 solamente
        ocr = AzureOCRProcessor()
        result = await ocr.process_document(str(file_path), max_pages=1)
        if result:
            persist_ocr(doc_id, result.to_dict(), "azure", "sample")
            return result.text[:max_chars]
    
    return ""

def _determine_route_for_file(file_path: Path) -> str:
    """Determina ruta de procesamiento para un archivo"""
    ext = file_path.suffix.lower()
    
    # Mapeo básico por extensión
    route_map = {
        '.pdf': 'ocr_text',      # PDFs generalmente necesitan OCR
        '.jpg': 'direct_ai',     # Imágenes van a visión
        '.jpeg': 'direct_ai',
        '.png': 'direct_ai',
        '.docx': 'ocr_text',
        '.xlsx': 'ocr_text',
        '.csv': 'ocr_text'
    }
    
    return route_map.get(ext, 'ocr_text')
```

## Fase B: Extracción Guiada y Renombrado Final

### B.1 Flujo Principal

```python
async def organize_documents_phase_b(
    staging_folder: Path,
    extract_all_fields: bool = False
) -> Path:
    """
    Fase B: Extracción guiada y renombrado final
    
    Args:
        staging_folder: Carpeta staging de Fase A
        extract_all_fields: Si False, solo extrae nombre_asegurado y numero_siniestro
    
    Returns:
        final_folder: Carpeta final renombrada
    """
    # 1. Cargar mapping
    mapping_file = staging_folder / "mapping.json"
    with open(mapping_file, "r", encoding="utf-8") as f:
        mapping = json.load(f)
    
    # 2. Preparar extractores
    ocr_processor = AzureOCRProcessor()
    ai_extractor = AIFieldExtractor()
    consolidator = AIConsolidator()
    
    # Campos objetivo
    if extract_all_fields:
        target_fields = ExtractionConfig.REQUIRED_FIELDS
    else:
        target_fields = ["nombre_asegurado", "numero_siniestro"]
    
    # 3. Ejecutar extracción guiada
    extractions = []
    
    for file_info in mapping["files"]:
        staged_path = Path(file_info["staged"])
        doc_type = file_info["document_type"]
        route = file_info["route"]
        
        logger.info(f"Procesando {staged_path.name} (tipo: {doc_type}, ruta: {route})")
        
        # Obtener OCR (con cache)
        doc_id, file_hash = ensure_document_registered(
            f"organize_{mapping['metadata']['timestamp']}", 
            str(staged_path)
        )
        
        ocr_result = try_get_cached_ocr(doc_id, file_hash)
        if not ocr_result:
            ocr_result = await ocr_processor.process_document(str(staged_path))
            if ocr_result:
                persist_ocr(doc_id, ocr_result.to_dict(), "azure", "full")
        
        # Extraer campos (usando sistema guiado existente)
        if ocr_result:
            extraction = await ai_extractor.extract_from_document(
                ocr_result=ocr_result,
                document_name=staged_path.name,
                document_type=doc_type
            )
            extractions.append(extraction)
            
            # Actualizar file_info con campos extraídos
            file_info["extracted_fields"] = extraction.extracted_fields
    
    # 4. Consolidar solo campos de cabecera
    consolidated = await consolidator.consolidate_extractions(
        extractions=extractions,
        guided_mode=True,  # Usar modo guiado
        target_fields=target_fields  # Solo los campos que necesitamos
    )
    
    # 5. Determinar nombre de carpeta final
    nombre_asegurado = consolidated.final_values.get("nombre_asegurado", "").strip()
    numero_siniestro = consolidated.final_values.get("numero_siniestro", "").strip()
    
    final_name = _generate_final_folder_name(nombre_asegurado, numero_siniestro)
    
    # 6. Renombrar carpeta staging a nombre final
    final_folder = staging_folder.parent.parent / final_name
    
    # Si ya existe, añadir sufijo incremental
    if final_folder.exists():
        counter = 2
        while True:
            test_folder = staging_folder.parent.parent / f"{final_name}_{counter}"
            if not test_folder.exists():
                final_folder = test_folder
                break
            counter += 1
    
    # Mover y renombrar
    shutil.move(str(staging_folder), str(final_folder))
    
    # 7. Actualizar mapping con ubicación final
    mapping["metadata"]["final_folder"] = str(final_folder)
    mapping["metadata"]["nombre_asegurado"] = nombre_asegurado
    mapping["metadata"]["numero_siniestro"] = numero_siniestro
    mapping["consolidated_fields"] = consolidated.final_values
    
    mapping_file_new = final_folder / "mapping.json"
    with open(mapping_file_new, "w", encoding="utf-8") as f:
        json.dump(mapping, f, indent=2, ensure_ascii=False)
    
    logger.info(f"Documentos organizados en: {final_folder}")
    return final_folder
```

### B.2 Helpers para Fase B

```python
def _generate_final_folder_name(
    nombre_asegurado: str, 
    numero_siniestro: str,
    max_length: int = 100
) -> str:
    """
    Genera nombre de carpeta final sanitizado
    Formato: <NOMBRE_ASEGURADO> - <NUMERO_SINIESTRO>
    """
    # Validar y limpiar nombre asegurado
    if nombre_asegurado:
        # Remover caracteres no seguros
        nombre_clean = re.sub(r'[<>:"/\\|?*]', '', nombre_asegurado)
        # Normalizar espacios
        nombre_clean = ' '.join(nombre_clean.split())
        # Convertir a mayúsculas
        nombre_clean = nombre_clean.upper()[:50]  # Max 50 chars
    else:
        nombre_clean = "DESCONOCIDO"
    
    # Validar número de siniestro (14 dígitos)
    if numero_siniestro:
        # Extraer solo dígitos
        siniestro_clean = ''.join(filter(str.isdigit, numero_siniestro))
        
        # Validar longitud
        if len(siniestro_clean) == 14:
            # Formato: mantener los 14 dígitos
            siniestro_clean = siniestro_clean
        else:
            # Si no es válido, usar como está (truncado)
            siniestro_clean = re.sub(r'[<>:"/\\|?*]', '', numero_siniestro)[:20]
    else:
        # Fallback: timestamp
        siniestro_clean = datetime.now().strftime("%Y%m%d%H%M%S")
    
    # Combinar
    folder_name = f"{nombre_clean} - {siniestro_clean}"
    
    # Truncar si es muy largo
    if len(folder_name) > max_length:
        # Priorizar el número de siniestro
        available = max_length - len(siniestro_clean) - 3  # " - "
        nombre_truncated = nombre_clean[:available]
        folder_name = f"{nombre_truncated} - {siniestro_clean}"
    
    return folder_name

def _sanitize_filename(filename: str, max_length: int = 150) -> str:
    """Sanitiza nombre de archivo para ser seguro en filesystem"""
    # Remover caracteres no seguros
    safe = re.sub(r'[<>:"/\\|?*]', '_', filename)
    
    # Limitar longitud
    if len(safe) > max_length:
        name, ext = os.path.splitext(safe)
        name = name[:max_length - len(ext)]
        safe = name + ext
    
    return safe
```

## Integración con run_report.py

### Modificaciones Necesarias

```python
# En scripts/run_report.py

def add_organize_arguments(parser):
    """Añade argumentos para organización de documentos"""
    parser.add_argument(
        "--organize-only",
        action="store_true",
        help="Solo ejecuta Fase A (clasificación y staging) y termina"
    )
    
    parser.add_argument(
        "--organize-first", 
        action="store_true",
        help="Ejecuta Fase A y B antes del pipeline normal"
    )
    
    parser.add_argument(
        "--skip-llm-classification",
        action="store_true",
        help="No usar LLM para clasificación, solo heurísticas"
    )
    
    parser.add_argument(
        "--extract-all-fields",
        action="store_true",
        help="En Fase B, extraer todos los campos (no solo cabecera)"
    )

async def main():
    parser = argparse.ArgumentParser(description="Fraud Scorer v2.0")
    # ... argumentos existentes ...
    add_organize_arguments(parser)
    args = parser.parse_args()
    
    # Si es modo organización
    if args.organize_only or args.organize_first:
        input_folder = Path(args.folder)
        
        # Fase A
        logger.info("=== FASE A: Clasificación y Staging ===")
        staging_folder, mapping = await organize_documents_phase_a(
            input_folder=input_folder,
            use_llm_fallback=not args.skip_llm_classification
        )
        logger.info(f"Staging completo: {staging_folder}")
        
        if args.organize_only:
            # Solo Fase A, terminar aquí
            print(f"\nDocumentos organizados en staging: {staging_folder}")
            print(f"Total archivos: {len(mapping['files'])}")
            
            # Mostrar resumen
            type_counts = {}
            for f in mapping['files']:
                dt = f['document_type']
                type_counts[dt] = type_counts.get(dt, 0) + 1
            
            print("\nDistribución por tipo:")
            for doc_type, count in sorted(type_counts.items()):
                print(f"  - {doc_type}: {count}")
            
            return
        
        # Fase B
        logger.info("=== FASE B: Extracción y Renombrado Final ===")
        final_folder = await organize_documents_phase_b(
            staging_folder=staging_folder,
            extract_all_fields=args.extract_all_fields
        )
        
        print(f"\nDocumentos organizados en: {final_folder}")
        
        if not args.organize_first:
            # Si solo era organización, terminar
            return
        
        # Continuar con pipeline normal usando la carpeta final
        args.folder = str(final_folder)
    
    # ... resto del pipeline existente ...
```

## Modificaciones Mínimas a Settings.py

```python
# En src/fraud_scorer/settings.py

class ExtractionConfig:
    # ... configuración existente ...
    
    # === NUEVA SECCIÓN: Configuración de Organización ===
    
    # Directorio base para uploads organizados
    UPLOADS_DIR = Path("data/uploads")
    
    # Directorio temporal para staging
    STAGING_DIR = UPLOADS_DIR / "renombre_de_documentos"
    
    # Alias de tipos de documento (mapeo corto → canónico)
    DOCUMENT_TYPE_ALIASES = {
        # Nombres cortos para archivos
        "poliza": "poliza_de_la_aseguradora",
        "informe": "informe_preliminar_del_ajustador",
        "reclamacion": "carta_de_reclamacion_formal_a_la_aseguradra",
        "investigacion": "carpeta_de_investigacion",
        "denuncia": "denuncia",  # Ya es corto
        "factura": "factura",    # Ya es corto
        "carta_porte": "carta_porte",
        "peritaje": "peritaje"
    }
    
    # Prioridades para ordenamiento de archivos
    DOCUMENT_PRIORITIES = {
        "informe_preliminar_del_ajustador": 1,
        "poliza_de_la_aseguradora": 2,
        "carta_de_reclamacion_formal_a_la_aseguradra": 3,
        "carpeta_de_investigacion": 4,
        "denuncia": 5,
        "factura": 6,
        "carta_porte": 7,
        "peritaje": 8,
        "otro": 99
    }
    
    # Parámetros de clasificación
    CLASSIFICATION_CONFIG = {
        "min_confidence_threshold": 0.6,  # Umbral para usar LLM
        "sample_text_length": 1500,       # Caracteres para clasificación
        "llm_model": "gpt-4o-mini",       # Modelo económico
        "llm_temperature": 0.1,           # Baja temperatura para consistencia
        "llm_max_tokens": 200             # Límite de tokens para respuesta
    }
    
    # Configuración de nombres de archivo
    FILE_NAMING_CONFIG = {
        "max_folder_length": 100,         # Longitud máxima carpeta
        "max_file_length": 150,           # Longitud máxima archivo
        "route_labels": {
            "ocr_text": "OCR",
            "direct_ai": "VIS"
        }
    }
```

## Testing y Validación

### Test de Fase A

```python
# test_phase_a.py

async def test_classification():
    """Test del clasificador de documentos"""
    classifier = DocumentClassifier()
    
    test_cases = [
        ("POLIZA_SEGUROS.pdf", "Esta es una póliza de seguros con vigencia desde..."),
        ("factura_123.pdf", "CFDI Factura\nSubtotal: $1000\nIVA: $160"),
        ("documento_random.txt", "Texto sin indicadores claros")
    ]
    
    for filename, content in test_cases:
        doc_type, confidence, reasons = await classifier.classify(
            content, filename, use_llm_fallback=False
        )
        print(f"{filename}: {doc_type} (conf: {confidence:.2f})")
        print(f"  Razones: {reasons}")

async def test_phase_a_pipeline():
    """Test completo de Fase A"""
    test_folder = Path("test_documents")
    staging, mapping = await organize_documents_phase_a(
        test_folder,
        use_llm_fallback=False  # Solo heurísticas para test
    )
    
    assert staging.exists()
    assert (staging / "mapping.json").exists()
    
    # Verificar nombres
    for file_info in mapping["files"]:
        staged = Path(file_info["staged"])
        assert staged.exists()
        assert "__" in staged.name  # Verificar formato
```

### Test de Fase B

```python
# test_phase_b.py

async def test_folder_naming():
    """Test de generación de nombres de carpeta"""
    
    test_cases = [
        ("MODA YKT S.A. DE C.V.", "20250000002494", "MODA YKT S.A. DE C.V. - 20250000002494"),
        ("", "20250000002494", "DESCONOCIDO - 20250000002494"),
        ("Nombre Muy Largo"*10, "12345678901234", None),  # Se truncará
        ("Asegurado<>:/", "12345678901234", "ASEGURADO - 12345678901234")  # Sanitizado
    ]
    
    for nombre, siniestro, expected in test_cases:
        result = _generate_final_folder_name(nombre, siniestro)
        if expected:
            assert result == expected, f"Expected {expected}, got {result}"
        assert len(result) <= 100  # Verificar longitud máxima
```

## Monitoreo y Métricas

```python
class OrganizationMetrics:
    """Métricas del proceso de organización"""
    
    def __init__(self):
        self.phase_a_stats = {
            "total_files": 0,
            "classified_heuristic": 0,
            "classified_llm": 0,
            "classification_time_ms": [],
            "types_distribution": {}
        }
        
        self.phase_b_stats = {
            "extraction_success": 0,
            "extraction_failures": 0,
            "fields_found": {
                "nombre_asegurado": 0,
                "numero_siniestro": 0
            },
            "total_time_ms": 0
        }
    
    def log_classification(self, doc_type: str, used_llm: bool, time_ms: int):
        self.phase_a_stats["total_files"] += 1
        if used_llm:
            self.phase_a_stats["classified_llm"] += 1
        else:
            self.phase_a_stats["classified_heuristic"] += 1
        
        self.phase_a_stats["classification_time_ms"].append(time_ms)
        
        # Distribución por tipo
        types = self.phase_a_stats["types_distribution"]
        types[doc_type] = types.get(doc_type, 0) + 1
    
    def generate_report(self) -> Dict[str, Any]:
        """Genera reporte de métricas"""
        avg_time = (
            sum(self.phase_a_stats["classification_time_ms"]) / 
            len(self.phase_a_stats["classification_time_ms"])
            if self.phase_a_stats["classification_time_ms"] else 0
        )
        
        return {
            "phase_a": {
                **self.phase_a_stats,
                "avg_classification_time_ms": avg_time,
                "llm_usage_rate": (
                    self.phase_a_stats["classified_llm"] / 
                    self.phase_a_stats["total_files"]
                    if self.phase_a_stats["total_files"] > 0 else 0
                )
            },
            "phase_b": self.phase_b_stats
        }
```

## Casos Límite y Manejo de Errores

### 1. Archivos No Soportados
```python
# En organize_documents_phase_a
unsupported_folder = staging_folder / "_no_soportados"
if unsupported_files:
    unsupported_folder.mkdir(exist_ok=True)
    for file in unsupported_files:
        shutil.copy2(file, unsupported_folder / file.name)
```

### 2. Errores de OCR
```python
try:
    ocr_result = await ocr_processor.process_document(str(file_path))
except Exception as e:
    logger.error(f"Error OCR en {file_path}: {e}")
    # Continuar con clasificación por nombre solamente
    doc_type = "otro"
```

### 3. Colisiones de Nombres
```python
def _handle_name_collision(base_path: Path) -> Path:
    """Maneja colisiones añadiendo sufijo incremental"""
    if not base_path.exists():
        return base_path
    
    counter = 2
    while True:
        new_path = base_path.parent / f"{base_path.stem}_{counter}{base_path.suffix}"
        if not new_path.exists():
            return new_path
        counter += 1
```

## Línea de Comandos

### Ejemplos de Uso

```bash
# Solo Fase A (clasificación y staging)
python scripts/run_report.py --folder ./documentos --organize-only

# Fase A + B (organización completa)
python scripts/run_report.py --folder ./documentos --organize-first

# Organización sin LLM (solo heurísticas)
python scripts/run_report.py --folder ./documentos --organize-first --skip-llm-classification

# Organización + extracción completa + reporte
python scripts/run_report.py --folder ./documentos --organize-first --extract-all-fields

# Pipeline normal (sin organización)
python scripts/run_report.py --folder ./documentos
```

## Resumen de Beneficios

1. **Reutilización máxima**: Usa componentes existentes (OCR, cache, extractores, validadores)
2. **Costo optimizado**: Fase A usa heurísticas primero, LLM solo si necesario
3. **Cache inteligente**: Reutiliza OCR entre fases
4. **Nombres informativos**: Archivos indican tipo y ruta de procesamiento
5. **Trazabilidad**: mapping.json mantiene historial completo
6. **Flexible**: Puede ejecutarse por fases o completo
7. **Robusto**: Maneja errores y casos límite gracefully

## Próximos Pasos

1. Implementar `DocumentClassifier` en `src/fraud_scorer/processors/classification.py`
2. Añadir funciones de organización a `scripts/run_report.py`
3. Actualizar `settings.py` con configuraciones nuevas
4. Crear tests unitarios y de integración
5. Documentar en README principal
6. Considerar API endpoints para organización async