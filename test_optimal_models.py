#!/usr/bin/env python3
"""
Test para verificar que los modelos √≥ptimos se seleccionan correctamente seg√∫n la investigaci√≥n
"""

import sys
from pathlib import Path

# A√±adir el src al path
project_root = Path(__file__).resolve().parent
sys.path.insert(0, str(project_root / "src"))

from fraud_scorer.settings import get_model_for_task, ModelType


def test_optimal_model_selection():
    """Test que verifica la selecci√≥n √≥ptima de modelos seg√∫n investigaci√≥n 2025"""
    print("üß™ TESTEO DE SELECCI√ìN √ìPTIMA DE MODELOS GPT-5")
    print("=" * 70)
    
    # Test cases basados en la investigaci√≥n
    test_cases = [
        # Extracci√≥n con AI Directo (visi√≥n) - debe usar GPT-5 Mini
        ("extraction", "direct_ai", "gpt-5-mini", "Visi√≥n: GPT-5 Mini (95% m√°s econ√≥mico, optimizado para extraction)"),
        
        # Extracci√≥n con OCR + AI (texto) - debe usar GPT-5 completo  
        ("extraction", "ocr_text", "gpt-5", "Texto: GPT-5 (272K tokens, ideal para documentos complejos)"),
        
        # Consolidaci√≥n - debe usar GPT-5 completo
        ("consolidation", "ocr_text", "gpt-5", "Consolidaci√≥n: GPT-5 (razonamiento complejo)"),
        
        # Generaci√≥n - debe usar GPT-5 Mini
        ("generation", "ocr_text", "gpt-5-mini", "Generaci√≥n: GPT-5 Mini (eficiente)"),
        
        # Fallback para tarea desconocida
        ("unknown_task", "ocr_text", "gpt-4o-mini", "Fallback: GPT-4o Mini (compatibilidad)"),
    ]
    
    print("Verificando selecci√≥n de modelos:")
    success_count = 0
    
    for task, route, expected_model, description in test_cases:
        try:
            actual_model = get_model_for_task(task, route)
            
            if actual_model == expected_model:
                success_count += 1
                print(f"‚úÖ {description}")
                print(f"   {task} + {route} ‚Üí {actual_model}")
            else:
                print(f"‚ùå {description}")
                print(f"   {task} + {route} ‚Üí {actual_model} (esperaba {expected_model})")
        except Exception as e:
            print(f"‚ùå {description}")
            print(f"   ERROR: {e}")
        
        print()
    
    # Resumen de modelos disponibles
    print("=" * 70)
    print("MODELOS DISPONIBLES EN EL SISTEMA:")
    print("=" * 70)
    
    models = [
        ("GPT-5 (Visi√≥n)", ModelType.GPT5_VISION.value, "Para AI directo con visi√≥n"),
        ("GPT-5 Mini (Visi√≥n)", ModelType.GPT5_VISION_MINI.value, "RECOMENDADO para extraction + visi√≥n"),
        ("GPT-5 Nano (Visi√≥n)", ModelType.GPT5_VISION_NANO.value, "Para tareas simples de clasificaci√≥n"),
        ("GPT-5", ModelType.GPT5.value, "Para texto + 272K context"),
        ("GPT-5 Mini", ModelType.GPT5_MINI.value, "Para generaci√≥n eficiente"),
        ("GPT-4o Mini (Fallback)", ModelType.EXTRACTOR.value, "Compatibilidad"),
    ]
    
    for name, model_id, usage in models:
        print(f"‚Ä¢ {name:25} | {model_id:15} | {usage}")
    
    # Resultados finales
    print("\n" + "=" * 70)
    print("VENTAJAS DE LA CONFIGURACI√ìN OPTIMIZADA:")
    print("=" * 70)
    print("‚Ä¢ üî• AI Directo: GPT-5 Mini es 95% M√ÅS ECON√ìMICO que GPT-5 est√°ndar")
    print("‚Ä¢ üìÑ OCR + AI: GPT-5 con 272K tokens (2x m√°s contexto que GPT-4o)")
    print("‚Ä¢ üß† Consolidaci√≥n: GPT-5 completo para razonamiento avanzado")
    print("‚Ä¢ ‚ö° Generaci√≥n: GPT-5 Mini para eficiencia en reportes")
    
    print(f"\nüìä Resultado: {success_count}/{len(test_cases)} configuraciones correctas")
    
    if success_count == len(test_cases):
        print("‚úÖ CONFIGURACI√ìN √ìPTIMA VERIFICADA")
        print("‚ú® El sistema usar√° los mejores modelos GPT-5 para cada tarea")
        return True
    else:
        print("‚ùå Hay configuraciones sub√≥ptimas")
        return False


if __name__ == "__main__":
    success = test_optimal_model_selection()
    sys.exit(0 if success else 1)